{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import pandas as pandas\n",
    "import seaborn as seaborn\n",
    "import matplotlib.pyplot as matplot\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T15:51:56.315426100Z",
     "start_time": "2024-09-04T15:51:55.697758100Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRIEF DESCRIPTION OF THE DATASET: \n",
      ".. _olivetti_faces_dataset:\n",
      "\n",
      "The Olivetti faces dataset\n",
      "--------------------------\n",
      "\n",
      "`This dataset contains a set of face images`_ taken between April 1992 and \n",
      "April 1994 at AT&T Laboratories Cambridge. The\n",
      ":func:`sklearn.datasets.fetch_olivetti_faces` function is the data\n",
      "fetching / caching function that downloads the data\n",
      "archive from AT&T.\n",
      "\n",
      ".. _This dataset contains a set of face images: https://cam-orl.co.uk/facedatabase.html\n",
      "\n",
      "As described on the original website:\n",
      "\n",
      "    There are ten different images of each of 40 distinct subjects. For some\n",
      "    subjects, the images were taken at different times, varying the lighting,\n",
      "    facial expressions (open / closed eyes, smiling / not smiling) and facial\n",
      "    details (glasses / no glasses). All the images were taken against a dark\n",
      "    homogeneous background with the subjects in an upright, frontal position \n",
      "    (with tolerance for some side movement).\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   =====================\n",
      "    Classes                                40\n",
      "    Samples total                         400\n",
      "    Dimensionality                       4096\n",
      "    Features            real, between 0 and 1\n",
      "    =================   =====================\n",
      "\n",
      "The image is quantized to 256 grey levels and stored as unsigned 8-bit \n",
      "integers; the loader will convert these to floating point values on the \n",
      "interval [0, 1], which are easier to work with for many algorithms.\n",
      "\n",
      "The \"target\" for this database is an integer from 0 to 39 indicating the\n",
      "identity of the person pictured; however, with only 10 examples per class, this\n",
      "relatively small dataset is more interesting from an unsupervised or\n",
      "semi-supervised perspective.\n",
      "\n",
      "The original dataset consisted of 92 x 112, while the version available here\n",
      "consists of 64x64 images.\n",
      "\n",
      "When using these images, please give credit to AT&T Laboratories Cambridge.\n",
      "\n",
      "\n",
      "ATTRIBUTES: \n",
      "['DESCR', 'data', 'images', 'target']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "feature_names",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:54\u001B[0m, in \u001B[0;36mBunch.__getattr__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:39\u001B[0m, in \u001B[0;36mBunch.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m     35\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deprecated_key_to_warnings[key],\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m     38\u001B[0m     )\n\u001B[1;32m---> 39\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'feature_names'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m X \u001B[38;5;241m=\u001B[39m of\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m     11\u001B[0m y \u001B[38;5;241m=\u001B[39m of\u001B[38;5;241m.\u001B[39mtarget\n\u001B[1;32m---> 12\u001B[0m fn \u001B[38;5;241m=\u001B[39m \u001B[43mof\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_names\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDATA:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     15\u001B[0m show_data \u001B[38;5;241m=\u001B[39m pandas\u001B[38;5;241m.\u001B[39mDataFrame(data\u001B[38;5;241m=\u001B[39mX, columns\u001B[38;5;241m=\u001B[39mfn)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:56\u001B[0m, in \u001B[0;36mBunch.__getattr__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[key]\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m---> 56\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(key)\n",
      "\u001B[1;31mAttributeError\u001B[0m: feature_names"
     ]
    }
   ],
   "source": [
    "of = fetch_olivetti_faces()\n",
    "print(\"BRIEF DESCRIPTION OF THE DATASET: \")\n",
    "print(of.DESCR)\n",
    "print()\n",
    "\n",
    "print(\"ATTRIBUTES: \")\n",
    "print(dir(of))\n",
    "print()\n",
    "\n",
    "X = of.data\n",
    "y = of.target\n",
    "fn = of.feature_names\n",
    "\n",
    "print(\"DATA:\")\n",
    "show_data = pandas.DataFrame(data=X, columns=fn)\n",
    "show_data['type'] = pandas.Series(of.target)\n",
    "show_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T15:52:08.134180800Z",
     "start_time": "2024-09-04T15:52:07.319204100Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Further data analysis**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       mean radius  mean texture  mean perimeter    mean area  \\\ncount   569.000000    569.000000      569.000000   569.000000   \nmean     14.127292     19.289649       91.969033   654.889104   \nstd       3.524049      4.301036       24.298981   351.914129   \nmin       6.981000      9.710000       43.790000   143.500000   \n25%      11.700000     16.170000       75.170000   420.300000   \n50%      13.370000     18.840000       86.240000   551.100000   \n75%      15.780000     21.800000      104.100000   782.700000   \nmax      28.110000     39.280000      188.500000  2501.000000   \n\n       mean smoothness  mean compactness  mean concavity  mean concave points  \\\ncount       569.000000        569.000000      569.000000           569.000000   \nmean          0.096360          0.104341        0.088799             0.048919   \nstd           0.014064          0.052813        0.079720             0.038803   \nmin           0.052630          0.019380        0.000000             0.000000   \n25%           0.086370          0.064920        0.029560             0.020310   \n50%           0.095870          0.092630        0.061540             0.033500   \n75%           0.105300          0.130400        0.130700             0.074000   \nmax           0.163400          0.345400        0.426800             0.201200   \n\n       mean symmetry  mean fractal dimension  ...  worst texture  \\\ncount     569.000000              569.000000  ...     569.000000   \nmean        0.181162                0.062798  ...      25.677223   \nstd         0.027414                0.007060  ...       6.146258   \nmin         0.106000                0.049960  ...      12.020000   \n25%         0.161900                0.057700  ...      21.080000   \n50%         0.179200                0.061540  ...      25.410000   \n75%         0.195700                0.066120  ...      29.720000   \nmax         0.304000                0.097440  ...      49.540000   \n\n       worst perimeter   worst area  worst smoothness  worst compactness  \\\ncount       569.000000   569.000000        569.000000         569.000000   \nmean        107.261213   880.583128          0.132369           0.254265   \nstd          33.602542   569.356993          0.022832           0.157336   \nmin          50.410000   185.200000          0.071170           0.027290   \n25%          84.110000   515.300000          0.116600           0.147200   \n50%          97.660000   686.500000          0.131300           0.211900   \n75%         125.400000  1084.000000          0.146000           0.339100   \nmax         251.200000  4254.000000          0.222600           1.058000   \n\n       worst concavity  worst concave points  worst symmetry  \\\ncount       569.000000            569.000000      569.000000   \nmean          0.272188              0.114606        0.290076   \nstd           0.208624              0.065732        0.061867   \nmin           0.000000              0.000000        0.156500   \n25%           0.114500              0.064930        0.250400   \n50%           0.226700              0.099930        0.282200   \n75%           0.382900              0.161400        0.317900   \nmax           1.252000              0.291000        0.663800   \n\n       worst fractal dimension        type  \ncount               569.000000  569.000000  \nmean                  0.083946    0.627417  \nstd                   0.018061    0.483918  \nmin                   0.055040    0.000000  \n25%                   0.071460    0.000000  \n50%                   0.080040    1.000000  \n75%                   0.092080    1.000000  \nmax                   0.207500    1.000000  \n\n[8 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>...</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>14.127292</td>\n      <td>19.289649</td>\n      <td>91.969033</td>\n      <td>654.889104</td>\n      <td>0.096360</td>\n      <td>0.104341</td>\n      <td>0.088799</td>\n      <td>0.048919</td>\n      <td>0.181162</td>\n      <td>0.062798</td>\n      <td>...</td>\n      <td>25.677223</td>\n      <td>107.261213</td>\n      <td>880.583128</td>\n      <td>0.132369</td>\n      <td>0.254265</td>\n      <td>0.272188</td>\n      <td>0.114606</td>\n      <td>0.290076</td>\n      <td>0.083946</td>\n      <td>0.627417</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.524049</td>\n      <td>4.301036</td>\n      <td>24.298981</td>\n      <td>351.914129</td>\n      <td>0.014064</td>\n      <td>0.052813</td>\n      <td>0.079720</td>\n      <td>0.038803</td>\n      <td>0.027414</td>\n      <td>0.007060</td>\n      <td>...</td>\n      <td>6.146258</td>\n      <td>33.602542</td>\n      <td>569.356993</td>\n      <td>0.022832</td>\n      <td>0.157336</td>\n      <td>0.208624</td>\n      <td>0.065732</td>\n      <td>0.061867</td>\n      <td>0.018061</td>\n      <td>0.483918</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6.981000</td>\n      <td>9.710000</td>\n      <td>43.790000</td>\n      <td>143.500000</td>\n      <td>0.052630</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.106000</td>\n      <td>0.049960</td>\n      <td>...</td>\n      <td>12.020000</td>\n      <td>50.410000</td>\n      <td>185.200000</td>\n      <td>0.071170</td>\n      <td>0.027290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.156500</td>\n      <td>0.055040</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>11.700000</td>\n      <td>16.170000</td>\n      <td>75.170000</td>\n      <td>420.300000</td>\n      <td>0.086370</td>\n      <td>0.064920</td>\n      <td>0.029560</td>\n      <td>0.020310</td>\n      <td>0.161900</td>\n      <td>0.057700</td>\n      <td>...</td>\n      <td>21.080000</td>\n      <td>84.110000</td>\n      <td>515.300000</td>\n      <td>0.116600</td>\n      <td>0.147200</td>\n      <td>0.114500</td>\n      <td>0.064930</td>\n      <td>0.250400</td>\n      <td>0.071460</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.370000</td>\n      <td>18.840000</td>\n      <td>86.240000</td>\n      <td>551.100000</td>\n      <td>0.095870</td>\n      <td>0.092630</td>\n      <td>0.061540</td>\n      <td>0.033500</td>\n      <td>0.179200</td>\n      <td>0.061540</td>\n      <td>...</td>\n      <td>25.410000</td>\n      <td>97.660000</td>\n      <td>686.500000</td>\n      <td>0.131300</td>\n      <td>0.211900</td>\n      <td>0.226700</td>\n      <td>0.099930</td>\n      <td>0.282200</td>\n      <td>0.080040</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>15.780000</td>\n      <td>21.800000</td>\n      <td>104.100000</td>\n      <td>782.700000</td>\n      <td>0.105300</td>\n      <td>0.130400</td>\n      <td>0.130700</td>\n      <td>0.074000</td>\n      <td>0.195700</td>\n      <td>0.066120</td>\n      <td>...</td>\n      <td>29.720000</td>\n      <td>125.400000</td>\n      <td>1084.000000</td>\n      <td>0.146000</td>\n      <td>0.339100</td>\n      <td>0.382900</td>\n      <td>0.161400</td>\n      <td>0.317900</td>\n      <td>0.092080</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>28.110000</td>\n      <td>39.280000</td>\n      <td>188.500000</td>\n      <td>2501.000000</td>\n      <td>0.163400</td>\n      <td>0.345400</td>\n      <td>0.426800</td>\n      <td>0.201200</td>\n      <td>0.304000</td>\n      <td>0.097440</td>\n      <td>...</td>\n      <td>49.540000</td>\n      <td>251.200000</td>\n      <td>4254.000000</td>\n      <td>0.222600</td>\n      <td>1.058000</td>\n      <td>1.252000</td>\n      <td>0.291000</td>\n      <td>0.663800</td>\n      <td>0.207500</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:30:54.106523700Z",
     "start_time": "2024-08-18T00:30:54.055467100Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:30:57.244195500Z",
     "start_time": "2024-08-18T00:30:57.194418800Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:30:58.322657900Z",
     "start_time": "2024-08-18T00:30:58.201930400Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**PERCEPTRON**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def perceptron(X, y, margin=0.1, learning_rate=1.0, max_iters=200):\n",
    "    N, D = X.shape\n",
    "    Y = np.unique(y)\n",
    "    C = Y.size\n",
    "    weights = np.zeros((1+D, C))\n",
    "    \n",
    "    for iteration in range(1, max_iters + 1):\n",
    "        errors = 0\n",
    "        \n",
    "        for n in range(N):\n",
    "            xn = np.array([1, *X[n, :]])\n",
    "            cn = np.squeeze(np.where(Y==y[n]))\n",
    "            gn = weights[:,cn].T @ xn\n",
    "            err = False\n",
    "            \n",
    "            for c in np.arange(C):\n",
    "                if c != cn and weights[:,c].T @ xn + margin >= gn:\n",
    "                    weights[:, c] = weights[:, c] - learning_rate*xn; err = True\n",
    "            if err:\n",
    "                weights[:, cn] = weights[:, cn] + learning_rate*xn\n",
    "                errors = errors + 1\n",
    "                \n",
    "        if errors == 0:\n",
    "            break\n",
    "\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:30:59.940609900Z",
     "start_time": "2024-08-18T00:30:59.936096700Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.3% with margin 0.01, learning rate 0.1 and 1000 maximum iterations\n",
      "6min 54s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "best_precision = 0\n",
    "best_params = [0,0,0]\n",
    "\n",
    "for margin in (0.01, 0.1, 0.3, 0.5, 0.7):\n",
    "    for learning_rate in (0.1, 0.5, 1, 2):\n",
    "        for max_iters in (100, 200, 500, 1000):\n",
    "            W = perceptron(X_train, y_train, margin, learning_rate, max_iters)\n",
    "\n",
    "            X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n",
    "            y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n",
    "            err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "            precision = 100 - err_test\n",
    "            \n",
    "            if precision > best_precision:\n",
    "                best_precision = precision\n",
    "                best_params = [margin, learning_rate, max_iters]\n",
    "\n",
    "print(f'Accuracy: {best_precision:.1f}% with margin {best_params[0]}, learning rate {best_params[1]} and {best_params[2]} maximum iterations')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:37:56.213682600Z",
     "start_time": "2024-08-18T00:31:01.808516400Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NAIVE BAYES** "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.6% con {'var_smoothing': 1e-09}\n",
      "122 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "nb = GaussianNB()\n",
    "\n",
    "Gnb = {\"var_smoothing\": [1e-9, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "GSnb = GridSearchCV(nb, Gnb, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = GSnb.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f'Accuracy: {acc:.1%} con {GSnb.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:38:00.244236500Z",
     "start_time": "2024-08-18T00:38:00.104237300Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "**LINEAR DISCRIMINANT ANALYSIS**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.9% con {'n_components': 1, 'solver': 'svd', 'tol': 0.3}\n",
      "1.17 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "Glda = {\"solver\": ['svd', 'lsqr', 'eigen'], \"n_components\": [1,2], \"tol\": [1e-5, 1e-4, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "GSlda = GridSearchCV(lda, Glda, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = GSlda.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f'Accuracy: {acc:.1%} con {GSlda.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:38:47.445435600Z",
     "start_time": "2024-08-18T00:38:46.263248Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "**QUADRATIC DISCRIMINANT ANALYSIS**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.2% con {'tol': 1e-05}\n",
      "128 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "Gqda = {\"tol\": [1e-5, 1e-4, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "GSqda = GridSearchCV(qda, Gqda, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = GSqda.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f'Accuracy: {acc:.1%} con {GSqda.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:38:54.754475Z",
     "start_time": "2024-08-18T00:38:54.610360600Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "**LOGISTIC REGRESSION**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.5% con {'C': 100, 'max_iter': 50, 'multi_class': 'auto', 'n_jobs': 1, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.01}\n",
      "7min 47s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "log_reg = LogisticRegression(random_state=23)\n",
    "\n",
    "Glogreg = {\"penalty\": ['l1', 'l2', None], \"tol\": [0.001, 0.01, 0.1], \"solver\": ['lbfgs', 'liblinear', 'newton-cg'], \"multi_class\": ['auto', 'multinomial'], \"max_iter\": [10, 50, 100], \"n_jobs\": [1,2,4], \"C\": [0.001, 0.1, 1, 10, 100]}\n",
    "GSlogreg = GridSearchCV(log_reg, Glogreg, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = GSlogreg.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f'Accuracy: {acc:.1%} con {GSlogreg.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:46:53.270306400Z",
     "start_time": "2024-08-18T00:39:05.975433800Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "**DECISION TREE CLASSIFIER**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.6% con {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2}\n",
      "5.65 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=23)\n",
    "\n",
    "Gdtc = {\"criterion\": ['gini', 'entropy', 'log_loss'], \"max_depth\": [1,3,5,7,10,20], \"min_samples_split\": [2,3,4,5]}\n",
    "GSdtc = GridSearchCV(dtc, Gdtc, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = GSdtc.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f'Accuracy: {acc:.1%} con {GSdtc.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:47:04.706215500Z",
     "start_time": "2024-08-18T00:46:59.046999Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "**RANDOM FOREST**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.2% con {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 25}\n",
      "3min 11s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=23)\n",
    "\n",
    "Grfc = {\"n_estimators\": [1,5,10,25,50], \"criterion\": ['gini', 'entropy', 'log_loss'], \"bootstrap\": [True,False], \"max_depth\": [1,3,5,7,10,20], \"min_samples_split\": [2,3,4,5]}\n",
    "GSrfc = GridSearchCV(rfc, Grfc, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = GSrfc.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f'Accuracy: {acc:.1%} con {GSrfc.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T00:50:19.118376100Z",
     "start_time": "2024-08-18T00:47:07.272488800Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ADABOOST CLASSIFIER**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.4% con {'learning_rate': 1, 'n_estimators': 100}\n",
      "39.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "abc = AdaBoostClassifier(random_state=23)\n",
    "\n",
    "Gabc = {\"n_estimators\": [2,5,10,20,50,100], \"learning_rate\": [0.01,0.1,0.3,0.5,0.7,0.9,0.99,1]}\n",
    "GSabc = GridSearchCV(abc, Gabc, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = (GSabc.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(f'Accuracy: {acc:.1%} con {GSabc.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T03:30:20.883008500Z",
     "start_time": "2024-08-18T03:29:41.554460Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GRADIENT BOOSTING CLASSIFIER**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.7% con {'criterion': 'friedman_mse', 'learning_rate': 0.5, 'max_depth': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "16min 57s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=23)\n",
    "\n",
    "Ggbc = {\"n_estimators\": [2,5,10,20,50], \"learning_rate\": [0.01,0.1,0.3,0.5,0.7,0.9], \"criterion\": ['friedman_mse', 'squared_error'], \"max_depth\": [1,3,5,7,10], \"min_samples_split\": [2,3,4]}\n",
    "GSgbc = GridSearchCV(gbc, Ggbc, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = (GSgbc.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(f'Accuracy: {acc:.1%} con {GSgbc.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:19:39.473729400Z",
     "start_time": "2024-08-18T04:02:42.384179500Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "**KNEIGHBORS CLASSIFIER**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.5% con {'leaf_size': 5, 'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}\n",
      "26.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "Gknc = {'n_neighbors': [1,2,5,10,20], 'weights':['uniform', 'distance'], 'leaf_size': [5,10,20,30,50], 'p': [1,2,3,4,5]}\n",
    "GSknc = GridSearchCV(knc, Gknc, scoring='accuracy', refit=True, cv=5)\n",
    "\n",
    "acc = (GSknc.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(f'Accuracy: {acc:.1%} con {GSknc.best_params_}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-18T04:30:33.621459600Z",
     "start_time": "2024-08-18T04:30:07.335966200Z"
    }
   },
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
